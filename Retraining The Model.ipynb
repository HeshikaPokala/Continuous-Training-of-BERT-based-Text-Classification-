{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e3bc856-80e1-426d-98bb-8430b599525b",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using BERT with Single Text and CSV Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a2fcc4c-ee7e-4105-aed6-1d378a0799bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 'text' for single text input or 'csv' for batch processing from a CSV file:  csv\n",
      "Enter the path to your CSV file:  C:\\Users\\kumar_lf3uub3\\Desktop\\CloudSEK\\PS-1\\csv_testing_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reviews: 100%|████████████████████████████████████████████████████████████| 989/989 [11:04<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to csv_reviews.csv.\n",
      "Reviews saved to .\\csv_reviews.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the pre-trained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(r\"./bert_tokenizer\")\n",
    "model = BertForSequenceClassification.from_pretrained(r\"./bert_model\")\n",
    "model.eval()\n",
    "\n",
    "# Function to make predictions\n",
    "def predict_sentiment(input_data):\n",
    "    inputs = tokenizer(input_data, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    return predicted_class\n",
    "\n",
    "# Choose input type: text or CSV\n",
    "input_type = input(\"Enter 'text' for single text input or 'csv' for batch processing from a CSV file: \").strip().lower()\n",
    "\n",
    "if input_type == 'text':\n",
    "    # Single text input\n",
    "    input_text = input(\"Enter your review: \")\n",
    "    predicted_class = predict_sentiment(input_text)\n",
    "    sentiment = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
    "    print(f\"Predicted sentiment: {sentiment} (Class: {predicted_class})\")\n",
    "\n",
    "    # Save the review to a CSV file in append mode\n",
    "    review_records = [input_text]\n",
    "    output_review_csv_path = r\"./text_review.csv\"\n",
    "    review_df = pd.DataFrame(review_records, columns=['review'])\n",
    "    review_df.to_csv(output_review_csv_path, mode='a', index=False, header=not pd.io.common.file_exists(output_review_csv_path))\n",
    "    print(f\"Review saved to {output_review_csv_path}.\")\n",
    "\n",
    "elif input_type == 'csv':\n",
    "    # Batch processing from CSV\n",
    "    input_csv_path = input(\"Enter the path to your CSV file: \").strip()\n",
    "    reviews_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Check for either 'review' or 'reviews' column\n",
    "    review_column = 'review' if 'review' in reviews_df.columns else 'reviews' if 'reviews' in reviews_df.columns else None\n",
    "    if not review_column:\n",
    "        raise ValueError(\"The input CSV must contain either a 'review' or 'reviews' column.\")\n",
    "\n",
    "    # Initialize a list to store predictions\n",
    "    predicted_labels = []\n",
    "    review_records = []  # To store the reviews for CSV saving\n",
    "\n",
    "    # Batch prediction\n",
    "    for review in tqdm(reviews_df[review_column], desc=\"Processing reviews\"):\n",
    "        predicted_class = predict_sentiment(review)\n",
    "        predicted_labels.append(predicted_class)\n",
    "        review_records.append(review)  # Collect the review for saving\n",
    "\n",
    "    # Add predictions to DataFrame and save\n",
    "    reviews_df['predicted_label'] = predicted_labels\n",
    "    reviews_df['predicted_sentiment'] = reviews_df['predicted_label'].apply(lambda x: 'Positive' if x == 1 else 'Negative')\n",
    "\n",
    "    output_csv_path = r\"csv_reviews.csv\"\n",
    "    reviews_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}.\")\n",
    "\n",
    "    # Save only the reviews to a CSV file in append mode\n",
    "    output_review_csv_path = r\".\\csv_reviews.csv\"\n",
    "    review_df = pd.DataFrame(review_records, columns=['review'])\n",
    "    review_df.to_csv(output_review_csv_path, mode='a', index=False, header=not pd.io.common.file_exists(output_review_csv_path))\n",
    "    print(f\"Reviews saved to {output_review_csv_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3fa4c5-0cb8-4266-b8e2-6ee72a13bf21",
   "metadata": {},
   "source": [
    "# Data Drift Detection Between Training Data And New Data ALong With Confidence Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27e2fe3e-eeab-4cac-b8b1-94eeb4d85bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings and starting data drift calculation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|███████████████████████████████████████████████████████| 1978/1978 [21:36<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings added to existing file: csv_reviews.csv\n",
      "Drift Score: 1.1968214809894562\n",
      "Significant drift detected. Generating predictions for reviews.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting labels: 100%|███████████████████████████████████████████████████████████| 1978/1978 [19:58<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated review predictions saved to csv_reviews.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# Function to safely parse embeddings from strings\n",
    "def parse_embedding(embedding_str):\n",
    "    # Ensure the string is treated as a list of floats\n",
    "    return list(map(float, embedding_str.strip(\"[]\").split()))\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(reviews):\n",
    "    embeddings = []\n",
    "    for review in tqdm(reviews, desc=\"Generating embeddings\"):\n",
    "        inputs = tokenizer(review, return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            embedding = logits.numpy().flatten()  # Flatten logits as embedding\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "# Load reviews from 'csv_reviews.csv'\n",
    "input_csv_path = r\"csv_reviews.csv\"\n",
    "review_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Check if there are at least 900 reviews\n",
    "if len(review_df) >= 900:\n",
    "    print(\"Generating embeddings and starting data drift calculation...\")\n",
    "\n",
    "    # Generate embeddings for reviews\n",
    "    review_embeddings = generate_embeddings(review_df['review'].tolist())\n",
    "    review_df['embedding'] = review_embeddings\n",
    "\n",
    "    # Save updated 'csv_reviews.csv' with embeddings\n",
    "    review_df.to_csv(input_csv_path, index=False)\n",
    "    print(f\"Embeddings added to existing file: {input_csv_path}\")\n",
    "\n",
    "    # Load historical embeddings\n",
    "    historical_embeddings_csv_path = r\"embeddings.csv\"\n",
    "    historical_embeddings_df = pd.read_csv(historical_embeddings_csv_path)\n",
    "\n",
    "    # Parse stored embeddings from strings\n",
    "    historical_embeddings = historical_embeddings_df['embedding'].apply(parse_embedding).tolist()\n",
    "    current_embeddings = review_df['embedding'].tolist()\n",
    "\n",
    "    # Calculate mean embeddings\n",
    "    historical_mean_embedding = torch.mean(torch.tensor(historical_embeddings), dim=0).numpy()\n",
    "    current_mean_embedding = torch.mean(torch.tensor(current_embeddings), dim=0).numpy()\n",
    "\n",
    "    # Calculate Wasserstein Distance for drift detection\n",
    "    drift_score = wasserstein_distance(historical_mean_embedding, current_mean_embedding)\n",
    "    print(f\"Drift Score: {drift_score}\")\n",
    "\n",
    "    # Optional: Threshold for drift detection\n",
    "    threshold = 1\n",
    "    if drift_score > threshold:\n",
    "        print(\"Significant drift detected. Generating predictions for reviews.\")\n",
    "\n",
    "        # Initialize lists for predictions and confidence scores\n",
    "        predicted_labels = []\n",
    "        confidence_scores = []\n",
    "\n",
    "        # Predict labels and confidence scores\n",
    "        for review in tqdm(review_df['review'], desc=\"Predicting labels\"):\n",
    "            inputs = tokenizer(review, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probabilities = torch.softmax(logits, dim=1)\n",
    "                confidence, predicted_class = torch.max(probabilities, dim=1)\n",
    "                predicted_labels.append(predicted_class.item())\n",
    "                confidence_scores.append(confidence.item())\n",
    "\n",
    "        # Add labels and confidence scores to DataFrame\n",
    "        review_df['predicted_label'] = predicted_labels\n",
    "        review_df['confidence_score'] = confidence_scores\n",
    "\n",
    "        # Filter by confidence score >= 0.8\n",
    "        review_df = review_df[review_df['confidence_score'] >= 0.8]\n",
    "\n",
    "        # Save the retrained model\n",
    "        model.save_pretrained(\"./retrained_bert_model\")\n",
    "        tokenizer.save_pretrained(\"./retrained_bert_tokenizer\")\n",
    "\n",
    "        # Save the updated CSV\n",
    "        review_df.to_csv(input_csv_path, index=False)\n",
    "        print(f\"Updated review predictions saved to {input_csv_path}.\")\n",
    "    else:\n",
    "        print(\"No significant drift detected. No predictions needed.\")\n",
    "else:\n",
    "    print(f\"Number of reviews: {len(review_df)}. Waiting for at least 900 reviews.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844b6482-63fc-4375-867e-1c711f740fea",
   "metadata": {},
   "source": [
    "# Model Retraining Script Using New Data for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68bc1356-7865-4501-886d-19eeac380c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar_lf3uub3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training Epoch 1: 100%|██████████████████████████████████████████████████████████████| 203/203 [23:43<00:00,  7.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Training Loss: 0.0879277993966206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 23/23 [00:47<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Validation Loss: 0.02114323859669916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████████████████████████████████████████████████████████| 203/203 [23:44<00:00,  7.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Training Loss: 0.00633560698748947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 23/23 [00:48<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Validation Loss: 0.0020671287341468524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████████████████████████████████████████████████████████| 203/203 [23:46<00:00,  7.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Training Loss: 0.00182285575408906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 23/23 [00:49<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Validation Loss: 0.0009471750729616084\n",
      "Retraining completed and model saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load data\n",
    "data_path = \"csv_reviews.csv\"\n",
    "data_df = pd.read_csv(data_path)\n",
    "\n",
    "# Ensure the data has the necessary columns\n",
    "assert 'review' in data_df.columns and 'predicted_label' in data_df.columns, \"CSV must contain 'review' and 'predicted_label' columns.\"\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Define custom Dataset\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, tokenizer, max_length=128):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            review,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./bert_tokenizer\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"./bert_model\")\n",
    "model.train()\n",
    "\n",
    "# Prepare datasets and dataloaders\n",
    "train_dataset = ReviewDataset(train_df['review'].tolist(), train_df['predicted_label'].tolist(), tokenizer)\n",
    "val_dataset = ReviewDataset(val_df['review'].tolist(), val_df['predicted_label'].tolist(), tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "total_steps = len(train_loader) * 3  # 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, epochs=3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} | Training Loss: {total_train_loss / len(train_loader)}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} | Validation Loss: {total_val_loss / len(val_loader)}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, optimizer, scheduler)\n",
    "\n",
    "# Save the retrained model and tokenizer\n",
    "model.save_pretrained(\"./retrained_bert_model\")\n",
    "tokenizer.save_pretrained(\"./retrained_bert_tokenizer\")\n",
    "print(\"Retraining completed and model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c878948-3498-4413-942f-2e9d24537656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full code\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the pre-trained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(r\"./bert_tokenizer\")\n",
    "model = BertForSequenceClassification.from_pretrained(r\"./bert_model\")\n",
    "model.eval()\n",
    "\n",
    "# Function to make predictions\n",
    "def predict_sentiment(input_data):\n",
    "    inputs = tokenizer(input_data, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    return predicted_class\n",
    "\n",
    "# Choose input type: text or CSV\n",
    "input_type = input(\"Enter 'text' for single text input or 'csv' for batch processing from a CSV file: \").strip().lower()\n",
    "\n",
    "if input_type == 'text':\n",
    "    # Single text input\n",
    "    input_text = input(\"Enter your review: \")\n",
    "    predicted_class = predict_sentiment(input_text)\n",
    "    sentiment = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
    "    print(f\"Predicted sentiment: {sentiment} (Class: {predicted_class})\")\n",
    "\n",
    "    # Save the review to a CSV file in append mode\n",
    "    review_records = [input_text]\n",
    "    output_review_csv_path = r\"./text_review.csv\"\n",
    "    review_df = pd.DataFrame(review_records, columns=['review'])\n",
    "    review_df.to_csv(output_review_csv_path, mode='a', index=False, header=not pd.io.common.file_exists(output_review_csv_path))\n",
    "    print(f\"Review saved to {output_review_csv_path}.\")\n",
    "\n",
    "elif input_type == 'csv':\n",
    "    # Batch processing from CSV\n",
    "    input_csv_path = input(\"Enter the path to your CSV file: \").strip()\n",
    "    reviews_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Check for either 'review' or 'reviews' column\n",
    "    review_column = 'review' if 'review' in reviews_df.columns else 'reviews' if 'reviews' in reviews_df.columns else None\n",
    "    if not review_column:\n",
    "        raise ValueError(\"The input CSV must contain either a 'review' or 'reviews' column.\")\n",
    "\n",
    "    # Initialize a list to store predictions\n",
    "    predicted_labels = []\n",
    "    review_records = []  # To store the reviews for CSV saving\n",
    "\n",
    "    # Batch prediction\n",
    "    for review in tqdm(reviews_df[review_column], desc=\"Processing reviews\"):\n",
    "        predicted_class = predict_sentiment(review)\n",
    "        predicted_labels.append(predicted_class)\n",
    "        review_records.append(review)  # Collect the review for saving\n",
    "\n",
    "    # Add predictions to DataFrame and save\n",
    "    reviews_df['predicted_label'] = predicted_labels\n",
    "    reviews_df['predicted_sentiment'] = reviews_df['predicted_label'].apply(lambda x: 'Positive' if x == 1 else 'Negative')\n",
    "\n",
    "    output_csv_path = r\"csv_reviews.csv\"\n",
    "    reviews_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}.\")\n",
    "\n",
    "    # Save only the reviews to a CSV file in append mode\n",
    "    output_review_csv_path = r\".\\csv_reviews.csv\"\n",
    "    review_df = pd.DataFrame(review_records, columns=['review'])\n",
    "    review_df.to_csv(output_review_csv_path, mode='a', index=False, header=not pd.io.common.file_exists(output_review_csv_path))\n",
    "    print(f\"Reviews saved to {output_review_csv_path}.\")\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# Function to safely parse embeddings from strings\n",
    "def parse_embedding(embedding_str):\n",
    "    # Ensure the string is treated as a list of floats\n",
    "    return list(map(float, embedding_str.strip(\"[]\").split()))\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(reviews):\n",
    "    embeddings = []\n",
    "    for review in tqdm(reviews, desc=\"Generating embeddings\"):\n",
    "        inputs = tokenizer(review, return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            embedding = logits.numpy().flatten()  # Flatten logits as embedding\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "# Load reviews from 'csv_reviews.csv'\n",
    "input_csv_path = r\"csv_reviews.csv\"\n",
    "review_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Check if there are at least 900 reviews\n",
    "if len(review_df) >= 900:\n",
    "    print(\"Generating embeddings and starting data drift calculation...\")\n",
    "\n",
    "    # Generate embeddings for reviews\n",
    "    review_embeddings = generate_embeddings(review_df['review'].tolist())\n",
    "    review_df['embedding'] = review_embeddings\n",
    "\n",
    "    # Save updated 'csv_reviews.csv' with embeddings\n",
    "    review_df.to_csv(input_csv_path, index=False)\n",
    "    print(f\"Embeddings added to existing file: {input_csv_path}\")\n",
    "\n",
    "    # Load historical embeddings\n",
    "    historical_embeddings_csv_path = r\"embeddings.csv\"\n",
    "    historical_embeddings_df = pd.read_csv(historical_embeddings_csv_path)\n",
    "\n",
    "    # Parse stored embeddings from strings\n",
    "    historical_embeddings = historical_embeddings_df['embedding'].apply(parse_embedding).tolist()\n",
    "    current_embeddings = review_df['embedding'].tolist()\n",
    "\n",
    "    # Calculate mean embeddings\n",
    "    historical_mean_embedding = torch.mean(torch.tensor(historical_embeddings), dim=0).numpy()\n",
    "    current_mean_embedding = torch.mean(torch.tensor(current_embeddings), dim=0).numpy()\n",
    "\n",
    "    # Calculate Wasserstein Distance for drift detection\n",
    "    drift_score = wasserstein_distance(historical_mean_embedding, current_mean_embedding)\n",
    "    print(f\"Drift Score: {drift_score}\")\n",
    "\n",
    "    # Optional: Threshold for drift detection\n",
    "    threshold = 1\n",
    "    if drift_score > threshold:\n",
    "        print(\"Significant drift detected. Generating predictions for reviews.\")\n",
    "\n",
    "        # Initialize lists for predictions and confidence scores\n",
    "        predicted_labels = []\n",
    "        confidence_scores = []\n",
    "\n",
    "        # Predict labels and confidence scores\n",
    "        for review in tqdm(review_df['review'], desc=\"Predicting labels\"):\n",
    "            inputs = tokenizer(review, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probabilities = torch.softmax(logits, dim=1)\n",
    "                confidence, predicted_class = torch.max(probabilities, dim=1)\n",
    "                predicted_labels.append(predicted_class.item())\n",
    "                confidence_scores.append(confidence.item())\n",
    "\n",
    "        # Add labels and confidence scores to DataFrame\n",
    "        review_df['predicted_label'] = predicted_labels\n",
    "        review_df['confidence_score'] = confidence_scores\n",
    "\n",
    "        # Filter by confidence score >= 0.8\n",
    "        review_df = review_df[review_df['confidence_score'] >= 0.8]\n",
    "\n",
    "        # Save the retrained model\n",
    "        model.save_pretrained(\"./retrained_bert_model\")\n",
    "        tokenizer.save_pretrained(\"./retrained_bert_tokenizer\")\n",
    "\n",
    "        # Save the updated CSV\n",
    "        review_df.to_csv(input_csv_path, index=False)\n",
    "        print(f\"Updated review predictions saved to {input_csv_path}.\")\n",
    "    else:\n",
    "        print(\"No significant drift detected. No predictions needed.\")\n",
    "else:\n",
    "    print(f\"Number of reviews: {len(review_df)}. Waiting for at least 900 reviews.\")\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load data\n",
    "data_path = \"csv_reviews.csv\"\n",
    "data_df = pd.read_csv(data_path)\n",
    "\n",
    "# Ensure the data has the necessary columns\n",
    "assert 'review' in data_df.columns and 'predicted_label' in data_df.columns, \"CSV must contain 'review' and 'predicted_label' columns.\"\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Define custom Dataset\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, tokenizer, max_length=128):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            review,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./bert_tokenizer\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"./bert_model\")\n",
    "model.train()\n",
    "\n",
    "# Prepare datasets and dataloaders\n",
    "train_dataset = ReviewDataset(train_df['review'].tolist(), train_df['predicted_label'].tolist(), tokenizer)\n",
    "val_dataset = ReviewDataset(val_df['review'].tolist(), val_df['predicted_label'].tolist(), tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "total_steps = len(train_loader) * 3  # 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, epochs=3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} | Training Loss: {total_train_loss / len(train_loader)}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} | Validation Loss: {total_val_loss / len(val_loader)}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, optimizer, scheduler)\n",
    "\n",
    "# Save the retrained model and tokenizer\n",
    "model.save_pretrained(\"./retrained_bert_model\")\n",
    "tokenizer.save_pretrained(\"./retrained_bert_tokenizer\")\n",
    "print(\"Retraining completed and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ac942-a99a-4367-abf4-c6d2124d54e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
